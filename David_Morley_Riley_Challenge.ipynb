{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morleyd/morleyd.github.io/blob/master/David_Morley_Riley_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot4A8Ag2zOSb"
      },
      "source": [
        "# Riley Challenge\n",
        "David C. Morley\n",
        "\n",
        "This work seeks to apply the [TACRED relation classification dataset](https://nlp.stanford.edu/projects/tacred/) reported in [this paper](https://arxiv.org/abs/2010.01057) to the needs of Riley. It builds off pretrained models stored oh HuggingFace's Transformers. I use the [fine-tuned model checkpoint](https://huggingface.co/studio-ousia/luke-large-finetuned-tacred) from Luke, [\"{LUKE}: Deep Contextualized Entity Representations with Entity-aware Self-attention\"](https://aclanthology.org/2020.emnlp-main.523) for handling the relation extraction. The NER is handled by [SpanBERT](https://huggingface.co/mrm8488/spanbert-large-finetuned-tacred).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzHs0tI43I-l"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18lGoU-SmTK8",
        "outputId": "dbd8ea30-9fd2-41a8-8f92-4553fd883936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-m_hnhrzt\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-m_hnhrzt\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.11.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0.dev0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# Currently, LUKE is only available on the master branch\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBERtWDCnqXO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from tqdm import trange\n",
        "from transformers import LukeTokenizer, LukeForEntityPairClassification\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRywXM4azmCP"
      },
      "source": [
        "## Loading the dataset\n",
        "Download from source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLzX8LIS127b",
        "outputId": "da668272-b863-4d9e-9b26-88cf5264fec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-BJPEGPCkcEBgqlEq0WqvwECbOqM0bH3\n",
            "To: /content/tacred_LDC2018T24.tgz\n",
            "100% 62.1M/62.1M [00:00<00:00, 208MB/s]\n",
            "tacred/\n",
            "tacred/data/\n",
            "tacred/data/conll/\n",
            "tacred/data/conll/dev.conll\n",
            "tacred/data/conll/test.conll\n",
            "tacred/data/conll/train.conll\n",
            "tacred/data/gold/\n",
            "tacred/data/gold/test.gold\n",
            "tacred/data/gold/dev.gold\n",
            "tacred/data/gold/train.gold\n",
            "tacred/data/json/\n",
            "tacred/data/json/test.json\n",
            "tacred/data/json/dev.json\n",
            "tacred/data/json/train.json\n",
            "tacred/docs/\n",
            "tacred/docs/tacred_stats.tsv\n",
            "tacred/docs/README.md\n",
            "tacred/docs/file.tbl\n",
            "tacred/docs/zhang2017tacred.pdf\n",
            "tacred/tools/\n",
            "tacred/tools/generate_json.py\n",
            "tacred/tools/score.py\n",
            "tacred/index.html\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1-BJPEGPCkcEBgqlEq0WqvwECbOqM0bH3\n",
        "!tar xvzf tacred_LDC2018T24.tgz\n",
        "# Clean up.\n",
        "!rm tacred_LDC2018T24.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5sR9PDLtQLv"
      },
      "source": [
        "### Apply the patch from [TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task](https://github.com/DFKI-NLP/tacrev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDxHXpGHtLdG",
        "outputId": "02efc269-2868-47bb-93cf-b9fd220a6345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tacrev' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DFKI-NLP/tacrev\n",
        "# !pip install -r tacrev/requirements.txt  # only necessary for notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nh0PwI0tVfH",
        "outputId": "c17ce0c7-a344-46d8-e84d-0a06e7e6ebd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "06/17/2022 13:08:22 - INFO - __main__ - Number of examples in TACRED dataset: 22631\n",
            "06/17/2022 13:08:22 - INFO - __main__ - Number of unique relations in TACRED dataset: 42\n",
            "06/17/2022 13:08:22 - INFO - __main__ - Relation counts in TACRED dataset: [('no_relation', 17195), ('per:title', 919), ('org:top_members/employees', 534), ('per:employee_of', 375), ('org:alternate_names', 338), ('per:age', 243), ('per:countries_of_residence', 226), ('per:origin', 210), ('per:date_of_death', 206), ('per:cities_of_residence', 179), ('org:country_of_headquarters', 177), ('per:cause_of_death', 168), ('per:spouse', 159), ('per:city_of_death', 118), ('org:subsidiaries', 113), ('org:city_of_headquarters', 109), ('per:charges', 105), ('per:children', 99), ('org:parents', 96), ('org:website', 86), ('org:members', 85), ('per:other_family', 80), ('org:founded_by', 76), ('per:stateorprovinces_of_residence', 72), ('org:stateorprovince_of_headquarters', 70), ('per:parents', 56), ('org:shareholders', 55), ('per:religion', 53), ('per:schools_attended', 50), ('per:country_of_death', 46), ('per:stateorprovince_of_death', 41), ('org:founded', 38), ('per:alternate_names', 38), ('per:city_of_birth', 33), ('per:date_of_birth', 31), ('org:member_of', 31), ('per:siblings', 30), ('org:number_of_employees/members', 27), ('per:stateorprovince_of_birth', 26), ('per:country_of_birth', 20), ('org:political/religious_affiliation', 10), ('org:dissolved', 8)]\n",
            "06/17/2022 13:08:22 - INFO - __main__ - Number of overwritten ground truth relations: 1590\n",
            "06/17/2022 13:08:22 - INFO - __main__ - Number of unique relations in patched dataset: 42\n",
            "06/17/2022 13:08:22 - INFO - __main__ - Relation counts in patched dataset: [('no_relation', 17331), ('per:title', 969), ('org:top_members/employees', 528), ('per:employee_of', 407), ('org:alternate_names', 348), ('per:age', 245), ('per:origin', 240), ('org:country_of_headquarters', 195), ('per:date_of_death', 188), ('per:cause_of_death', 175), ('per:spouse', 155), ('per:countries_of_residence', 143), ('per:city_of_death', 116), ('per:charges', 115), ('per:cities_of_residence', 112), ('org:city_of_headquarters', 108), ('org:subsidiaries', 105), ('per:children', 105), ('org:website', 91), ('org:stateorprovince_of_headquarters', 84), ('org:parents', 81), ('org:founded_by', 75), ('org:members', 64), ('per:religion', 62), ('per:stateorprovinces_of_residence', 57), ('per:parents', 56), ('per:schools_attended', 50), ('per:country_of_death', 47), ('per:stateorprovince_of_death', 47), ('per:city_of_birth', 38), ('per:alternate_names', 37), ('per:stateorprovince_of_birth', 35), ('org:shareholders', 35), ('org:founded', 34), ('per:date_of_birth', 31), ('per:siblings', 29), ('per:other_family', 26), ('org:number_of_employees/members', 24), ('per:country_of_birth', 21), ('org:political/religious_affiliation', 14), ('org:member_of', 7), ('org:dissolved', 1)]\n",
            "06/17/2022 13:08:22 - INFO - __main__ - !!! Relations not in patched dataset: set()\n",
            "06/17/2022 13:08:26 - INFO - __main__ - Number of examples in re-read dataset: 22631\n",
            "06/17/2022 13:08:26 - INFO - __main__ - Number of unique relations in re-read dataset: 42\n",
            "06/17/2022 13:08:27 - INFO - __main__ - Number of examples in TACRED dataset: 15509\n",
            "06/17/2022 13:08:27 - INFO - __main__ - Number of unique relations in TACRED dataset: 42\n",
            "06/17/2022 13:08:27 - INFO - __main__ - Relation counts in TACRED dataset: [('no_relation', 12184), ('per:title', 500), ('org:top_members/employees', 346), ('per:employee_of', 264), ('org:alternate_names', 213), ('per:age', 200), ('per:cities_of_residence', 189), ('per:countries_of_residence', 148), ('per:origin', 132), ('org:country_of_headquarters', 108), ('per:charges', 103), ('per:parents', 88), ('org:city_of_headquarters', 82), ('per:stateorprovinces_of_residence', 81), ('org:founded_by', 68), ('per:spouse', 66), ('org:parents', 62), ('per:other_family', 60), ('per:siblings', 55), ('per:date_of_death', 54), ('per:cause_of_death', 52), ('org:stateorprovince_of_headquarters', 51), ('per:religion', 47), ('org:subsidiaries', 44), ('per:children', 37), ('org:founded', 37), ('org:members', 31), ('per:schools_attended', 30), ('per:city_of_death', 28), ('org:website', 26), ('org:number_of_employees/members', 19), ('org:member_of', 18), ('per:stateorprovince_of_death', 14), ('org:shareholders', 13), ('per:alternate_names', 11), ('org:political/religious_affiliation', 10), ('per:date_of_birth', 9), ('per:country_of_death', 9), ('per:stateorprovince_of_birth', 8), ('per:country_of_birth', 5), ('per:city_of_birth', 5), ('org:dissolved', 2)]\n",
            "06/17/2022 13:08:27 - INFO - __main__ - Number of overwritten ground truth relations: 936\n",
            "06/17/2022 13:08:27 - INFO - __main__ - Number of unique relations in patched dataset: 41\n",
            "06/17/2022 13:08:27 - INFO - __main__ - Relation counts in patched dataset: [('no_relation', 12386), ('per:title', 509), ('org:top_members/employees', 348), ('per:employee_of', 252), ('org:alternate_names', 245), ('per:age', 201), ('per:countries_of_residence', 130), ('per:cities_of_residence', 130), ('per:origin', 111), ('per:charges', 106), ('org:country_of_headquarters', 92), ('org:city_of_headquarters', 89), ('per:parents', 86), ('org:founded_by', 76), ('per:stateorprovinces_of_residence', 73), ('per:spouse', 66), ('per:siblings', 59), ('org:parents', 57), ('org:stateorprovince_of_headquarters', 51), ('per:cause_of_death', 47), ('per:religion', 43), ('per:date_of_death', 42), ('per:children', 39), ('per:other_family', 37), ('org:founded', 37), ('org:subsidiaries', 31), ('per:schools_attended', 31), ('org:website', 27), ('org:members', 16), ('per:city_of_death', 16), ('org:number_of_employees/members', 14), ('per:stateorprovince_of_death', 13), ('org:political/religious_affiliation', 10), ('per:country_of_death', 10), ('per:date_of_birth', 7), ('per:stateorprovince_of_birth', 7), ('per:city_of_birth', 6), ('org:member_of', 4), ('org:shareholders', 3), ('org:dissolved', 1), ('per:alternate_names', 1)]\n",
            "06/17/2022 13:08:27 - INFO - __main__ - !!! Relations not in patched dataset: {'per:country_of_birth'}\n",
            "06/17/2022 13:08:30 - INFO - __main__ - Number of examples in re-read dataset: 15509\n",
            "06/17/2022 13:08:30 - INFO - __main__ - Number of unique relations in re-read dataset: 41\n"
          ]
        }
      ],
      "source": [
        "!python tacrev/scripts/apply_tacred_patch.py \\\n",
        "  --dataset-file ./tacred/data/json/dev.json \\\n",
        "  --patch-file ./tacrev/patch/dev_patch.json \\\n",
        "  --output-file ./tacred/data/json/dev_rev.json\n",
        "\n",
        "!python tacrev/scripts/apply_tacred_patch.py \\\n",
        "  --dataset-file ./tacred/data/json/test.json \\\n",
        "  --patch-file ./tacrev/patch/test_patch.json \\\n",
        "  --output-file ./tacred/data/json/test_rev.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9KN_AuFui2L"
      },
      "source": [
        "### Process Data\n",
        "\n",
        "Modified from [Luke's source code](https://github.com/studio-ousia/luke/blob/master/examples/relation_classification/reader.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-v01ix-i2FS"
      },
      "outputs": [],
      "source": [
        "def load_examples(dataset_file):\n",
        "    with open(dataset_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    examples = []\n",
        "    for i, item in enumerate(data):\n",
        "        tokens = item[\"token\"]\n",
        "        token_spans = dict(\n",
        "            subj=(item[\"subj_start\"], item[\"subj_end\"] + 1),\n",
        "            obj=(item[\"obj_start\"], item[\"obj_end\"] + 1)\n",
        "        )\n",
        "\n",
        "        if token_spans[\"subj\"][0] < token_spans[\"obj\"][0]:\n",
        "            entity_order = (\"subj\", \"obj\")\n",
        "        else:\n",
        "            entity_order = (\"obj\", \"subj\")\n",
        "\n",
        "        text = \"\"\n",
        "        cur = 0\n",
        "        char_spans = {}\n",
        "        for target_entity in entity_order:\n",
        "            token_span = token_spans[target_entity]\n",
        "            text += \" \".join(tokens[cur : token_span[0]])\n",
        "            if text:\n",
        "                text += \" \"\n",
        "            char_start = len(text)\n",
        "            text += \" \".join(tokens[token_span[0] : token_span[1]])\n",
        "            char_end = len(text)\n",
        "            char_spans[target_entity] = (char_start, char_end)\n",
        "            text += \" \"\n",
        "            cur = token_span[1]\n",
        "        text += \" \".join(tokens[cur:])\n",
        "        text = text.rstrip()\n",
        "\n",
        "        examples.append(dict(\n",
        "            text=text,\n",
        "            entity_spans=[tuple(char_spans[\"subj\"]), tuple(char_spans[\"obj\"])],\n",
        "            label=item[\"relation\"]\n",
        "        ))\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H6us8Unl231"
      },
      "outputs": [],
      "source": [
        "test_examples = load_examples(\"/content/tacred/data/json/test_rev.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0dLy9M1Tbdb"
      },
      "outputs": [],
      "source": [
        "# dataset_file = \"train.json\"\n",
        "# with open(dataset_file, \"r\") as f:\n",
        "#     data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHw1YtriA3S9"
      },
      "source": [
        "## Loading the fine-tuned model and tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKYVMCBWIrMr"
      },
      "source": [
        "### First get the relation extraction model\n",
        "We construct the model and tokenizer using the [fine-tuned model checkpoint](https://huggingface.co/studio-ousia/luke-large-finetuned-tacred)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9bXAEPZp0ZT",
        "outputId": "4ca3c037-3c8c-4e74-e1d8-668a783040e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-tacred were not used when initializing LukeForEntityPairClassification: ['luke.embeddings.position_ids']\n",
            "- This IS expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LukeForEntityPairClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load the model checkpoint\n",
        "rel_model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
        "rel_model.eval()\n",
        "rel_model.to(\"cuda\")\n",
        "\n",
        "# Load the tokenizer\n",
        "rel_tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGN6tG22SEIL"
      },
      "source": [
        "### Now get the NER model\n",
        "We construct the model and tokenizer using the [fine-tuned model checkpoint](https://huggingface.co/mrm8488/spanbert-large-finetuned-tacred)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0ePuwXpSEg4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
        "ner_model.eval()\n",
        "ner_model.to(\"cuda\")\n",
        "\n",
        "ner = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX-xzMbY2rWZ"
      },
      "source": [
        "## Putting both models together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6FjPCkGV1YV"
      },
      "outputs": [],
      "source": [
        "from itertools import permutations\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vXVUz2lO_3u"
      },
      "outputs": [],
      "source": [
        "Entity = namedtuple('Entity', ['e_type', 'span', 'text', 'score'])\n",
        "SPECIAL_TOKENS = {'FLA': '','LCB':'{', 'LRB':'(', 'LSB':'[', 'RCB':'}', 'RRB':')', 'RSB':']'}\n",
        "\n",
        "mean = lambda x: sum(x) / len(x)\n",
        "\n",
        "def clean_text(input_text: str) -> str:\n",
        "    for k,v in SPECIAL_TOKENS.items():\n",
        "        input_text = input_text.replace(f\"-{k}-\",v)\n",
        "    return input_text\n",
        "\n",
        "def merge_ents(ner_results: List[Dict]) -> List[Dict]:\n",
        "    ner_results = sorted(ner_results, key=lambda x: x['start'], reverse=False)\n",
        "    prev_e_type = ''\n",
        "    out = [{'entity':'', 'end': 0, 'start': 1, 'score':0, 'word':''}]\n",
        "    for entity in ner_results:\n",
        "        try:\n",
        "            e_loc, e_type = entity['entity'].split('-')\n",
        "            entity['entity'] = e_type\n",
        "            if e_loc == 'B':\n",
        "                out.append(entity)\n",
        "            elif e_loc == 'I':\n",
        "                out[-1]['end'] = entity['end']\n",
        "                out[-1]['word'] = ' '.join([out[-1]['word'], entity['word']])\n",
        "                out[-1]['score'] = mean([entity['score'], out[-1]['score']])\n",
        "            out[-1]['word'] = out[-1]['word'].replace(' ##', '')\n",
        "            if out[-1]['word'] in SPECIAL_TOKENS.keys():\n",
        "                out.pop()\n",
        "        except ValueError:\n",
        "            if entity['end'] > out[-1]['end']:\n",
        "                out.append(entity)\n",
        "    return out\n",
        "\n",
        "def ent_objects(entities: List[Dict]) -> List[Entity]:\n",
        "    out = []\n",
        "    for e in entities:\n",
        "        out.append(Entity(e['entity'], (e['start'], e['end']), e['word'], e['score']))\n",
        "    return out\n",
        "\n",
        "def ent_spans(entities: List[Dict]) -> List[Tuple]:\n",
        "    out = []\n",
        "    for e in entities:\n",
        "        if e['end'] - e['start'] > 1:\n",
        "            out.append((e['start'], e['end']))\n",
        "    return out\n",
        "\n",
        "def get_span(text: str, span: Tuple[int]) -> str:\n",
        "    return text[span[0]: span[1]]\n",
        "\n",
        "def extract_relations(text: str, ner_results: List[Dict]) -> List[Dict]:\n",
        "    out = []\n",
        "    entities = ent_objects(merge_ents(ner_results))\n",
        "    for e1, e2 in permutations(entities, 2):\n",
        "        inputs = rel_tokenizer(text, entity_spans=[e1.span, e2.span], return_tensors=\"pt\")\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "        outputs = rel_model(**inputs)\n",
        "\n",
        "        predicted_class_idx = outputs.logits.argmax(-1).item()\n",
        "        predicted_label = rel_model.config.id2label[predicted_class_idx]\n",
        "        out.append({'text': text, 'entity_spans': [e1.span, e2.span], 'label': predicted_label})\n",
        "    return out\n",
        "\n",
        "def get_all_spans(ner_results: List[List[Dict]]):  \n",
        "    pred_spans = []\n",
        "    for result in ner_results:\n",
        "        entities = ent_spans(merge_ents(result))\n",
        "        perms = [list(perm) for perm in permutations(entities, 2)]\n",
        "        pred_spans.append(perms)\n",
        "    return pred_spans "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuhdLhSlA93M"
      },
      "source": [
        "## Measuring performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkXNjJNuMb0X"
      },
      "source": [
        "### Base Model\n",
        "This reproduces the evaluation reported in the [original paper](https://arxiv.org/abs/2010.01057) is successfully reproduced. It classifies relations between entity pairs in the test set and measures the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGnyU7rGnNg2",
        "outputId": "d2cf6903-02f5-4121-b498-baf9ba249673"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 122/122 [07:42<00:00,  3.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "precision: 0.7661131438221221 recall: 0.8715978226064681 f1: 0.8154583582983823\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "num_predicted = 0\n",
        "num_gold = 0\n",
        "num_correct = 0\n",
        "\n",
        "for batch_start_idx in trange(0, len(test_examples), batch_size):\n",
        "    batch_examples = test_examples[batch_start_idx:batch_start_idx + batch_size]\n",
        "    texts = [example[\"text\"] for example in batch_examples]\n",
        "    entity_spans = [example[\"entity_spans\"] for example in batch_examples]\n",
        "    gold_labels = [example[\"label\"] for example in batch_examples]\n",
        "    \n",
        "    inputs = rel_tokenizer(texts, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = rel_model(**inputs)\n",
        "    predicted_indices = outputs.logits.argmax(-1)\n",
        "    predicted_labels = [rel_model.config.id2label[index.item()] for index in predicted_indices]\n",
        "    for predicted_label, gold_label in zip(predicted_labels, gold_labels):\n",
        "        if predicted_label != \"no_relation\":\n",
        "            num_predicted += 1\n",
        "        if gold_label != \"no_relation\":\n",
        "            num_gold += 1\n",
        "            if predicted_label == gold_label:\n",
        "                num_correct += 1\n",
        "\n",
        "precision = num_correct / num_predicted\n",
        "recall = num_correct / num_gold\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f\"\\n\\nprecision: {precision} recall: {recall} f1: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSRcpqUFMibl"
      },
      "source": [
        "### My Model\n",
        "The main difference here is that I mimic the data that you'll provide me by not including the spans of the entities. Instead, I use NER to predict the location of the entities and return all the relations between those entities. This has the benefit of giving more than one possible relation per utterance. Unfortunately, this also makes it have different results from the originial test set. I mitigate this by comparing the overlap of the spans that I predict with those provided. If the closest matching span (with at least half overlap) has the same relation, I count it as a positive example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "360d2sNxXHKU"
      },
      "outputs": [],
      "source": [
        "def span_overlap(s1: Tuple[int], s2: Tuple[int]) -> float:\n",
        "    # print(repr(s1))\n",
        "    b1, e1 = s1\n",
        "    b2, e2 = s2\n",
        "    # Check no overlap\n",
        "    if e1 < b2 or b1 > e2:\n",
        "        return 0.\n",
        "    else:\n",
        "        return abs(1 - abs((e2 - e1 + b2 - b1) / (max(e2, e1) - min(b2, b1))))\n",
        "\n",
        "def matching_spans(pred_spans: List[Dict], gold_spans: List[Tuple], k: int) -> List[int]:\n",
        "    \"\"\"For a list of predicted span dictionaries (output of extract_relations)\n",
        "    return the indices of those which most overlap with their gold counterparts\n",
        "    in order of most similar to least\n",
        "    \"\"\"\n",
        "    matches = [i for i, pred in enumerate(pred_spans) if span_overlap(pred[k], gold_spans[k]) > .5]\n",
        "    # return sorted(matches, key=lambda x: span_overlap(pred_spans[x]['entity_spans'][k], gold_spans[k]), reverse=True)\n",
        "    return matches\n",
        "\n",
        "def get_pred_label(pred_spans: Tuple[Tuple], gold_spans: List[Tuple], text):\n",
        "    global TICKER\n",
        "    beg_spans = matching_spans(pred_spans, gold_spans, 0)\n",
        "    end_spans = matching_spans(pred_spans, gold_spans, 1)\n",
        "    if not beg_spans or not end_spans:\n",
        "        TICKER += 1\n",
        "        return [\"no_relation\"]\n",
        "    else:\n",
        "        intersection = np.intersect1d(beg_spans, end_spans)\n",
        "        filtered_spans = [list(pred_spans[i]) for i in intersection]\n",
        "        n = len(filtered_spans)\n",
        "\n",
        "        try:\n",
        "            inputs = rel_tokenizer([text]*n, entity_spans=filtered_spans, return_tensors=\"pt\",padding=True,truncation=True)\n",
        "            inputs = inputs.to(\"cuda\")\n",
        "            with torch.no_grad():\n",
        "                outputs = rel_model(**inputs)\n",
        "\n",
        "            predicted_indices = outputs.logits.argmax(-1)\n",
        "            predicted_labels = [rel_model.config.id2label[index.item()] for index in predicted_indices]\n",
        "        except ValueError:\n",
        "            print(filtered_spans)\n",
        "            predicted_labels = [\"no_relation\"]\n",
        "        return predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMyNdihy_qK9"
      },
      "source": [
        "### First Model Eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En9gsNYmdpQc"
      },
      "source": [
        "Note the high precision and low recall. This is caused by my method of extracting spans. I only look for names, organizations and locations while TACRED's data set looks for more types of relations. However, those that I do select, are selected correctly. This number is taken only out of the samples that had a relation (~%20 of the total ammount). My true accuracy was actually much higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo_491o25aFf",
        "outputId": "649f26b7-f22e-4f29-bcde-1b6301727406"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 4/122 [00:11<05:39,  2.88s/it]/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py:1015: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  UserWarning,\n",
            "100%|██████████| 122/122 [06:00<00:00,  2.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "precision: 0.7490774907749077 recall: 0.45501120717259047 f1: 0.5661354581673307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "9846"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TICKER = 0\n",
        "batch_size = 128\n",
        "\n",
        "num_predicted = 0\n",
        "num_gold = 0\n",
        "num_correct = 0\n",
        "\n",
        "total = 0\n",
        "num_same = 0\n",
        "\n",
        "for batch_start_idx in trange(0, len(test_examples), batch_size):\n",
        "    batch_examples = test_examples[batch_start_idx:batch_start_idx + batch_size]\n",
        "    texts = [example[\"text\"] for example in batch_examples]\n",
        "    entity_spans = [example[\"entity_spans\"] for example in batch_examples]\n",
        "    gold_labels = [example[\"label\"] for example in batch_examples]\n",
        "\n",
        "    ner_results = ner(texts)\n",
        "    new_spans = get_all_spans(ner_results)\n",
        "\n",
        "    for pred_spans, gold_label, gold_span, text in zip(new_spans, gold_labels, entity_spans, texts):\n",
        "        total += 1\n",
        "        pred_labels = get_pred_label(pred_spans, gold_span, text)\n",
        "        if gold_label in pred_labels:\n",
        "            num_same += 1\n",
        "        if gold_label != \"no_relation\":\n",
        "            num_gold += 1\n",
        "            if gold_label in pred_labels:\n",
        "                num_correct += 1\n",
        "        num_predicted += sum(1 for label in pred_labels if label != \"no_relation\")\n",
        "\n",
        "precision = num_correct / num_predicted\n",
        "recall = num_correct / num_gold\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f\"\\n\\nprecision: {precision} recall: {recall} f1: {f1}\")\n",
        "TICKER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSW1P0C2eVrz"
      },
      "source": [
        "Approx. 86.5% of my predictions matched the gold sample even though almost 2/3 of my predictions defaulted to no relation because the spans I predicted weren't the ones that were annotated (correctly or otherwise). This is just an example of why this is a poor dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjJhTXj9TfCp",
        "outputId": "acc0e2c6-aed6-48bc-984c-9921e80fb3c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8651105809529951, 0.6348571797021084)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_same / total, TICKER/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut1RDvlb3BLx"
      },
      "source": [
        "## Sandbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSXk4ucreF61"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP26ZoC7Ne6S"
      },
      "outputs": [],
      "source": [
        "wrapper = textwrap.TextWrapper(width=100)\n",
        "\n",
        "def print_relations(text):\n",
        "    if not isinstance(text, list):\n",
        "        text = [text]\n",
        "    ner_results = ner(text)\n",
        "    new_spans = get_all_spans(ner_results)\n",
        "    for doc, result in zip(text, new_spans):\n",
        "        found_relation = False\n",
        "        print('-'*100)\n",
        "        print(wrapper.fill(clean_text(doc)))\n",
        "        print()\n",
        "        \n",
        "        n = len(result)\n",
        "        inputs = rel_tokenizer([doc]*n, entity_spans=result, return_tensors=\"pt\", padding=True)\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = rel_model(**inputs)\n",
        "\n",
        "        predicted_indices = outputs.logits.argmax(-1)\n",
        "        predicted_labels = [rel_model.config.id2label[index.item()] for index in predicted_indices]\n",
        "        for span, label in zip(result, predicted_labels):\n",
        "            if label != 'no_relation':\n",
        "                found_relation = True\n",
        "                print(f\"{repr(get_span(doc, span[0]))} -> {repr(get_span(doc, span[1]))}:\\t{label}\")\n",
        "\n",
        "        if not found_relation:\n",
        "            print(\"***No Relations Found***\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm6pq97dSsgZ",
        "outputId": "6b692c6d-e821-437e-ba30-4ef995ef0d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Tom Thabane resigned in October last year to form the All Basotho Convention ( ABC ) , crossing the\n",
            "floor with 17 members of parliament , causing constitutional monarch King Letsie III to dissolve\n",
            "parliament and call the snap election .\n",
            "\n",
            "'Tom Thabane' -> 'All Basotho':\tper:employee_of\n",
            "'All Basotho' -> 'Tom Thabane':\torg:founded_by\n",
            "'All Basotho' -> 'LRB':\torg:alternate_names\n",
            "'All Basotho' -> 'ABC':\torg:alternate_names\n",
            "'All Basotho' -> 'RRB':\torg:alternate_names\n",
            "'LRB' -> 'All Basotho':\torg:alternate_names\n",
            "'ABC' -> 'Tom Thabane':\torg:founded_by\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I just went for a jog and ran into Ben Fischer who is the chief revenue officer of XYZ company.\n",
            "\n",
            "'Ben Fischer' -> 'XYZ':\tper:employee_of\n",
            "'XYZ' -> 'Ben Fischer':\torg:top_members/employees\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Beyoncé lives in Los Angeles.\n",
            "\n",
            "'Beyoncé' -> 'Los Angeles':\tper:cities_of_residence\n",
            "'Los Angeles' -> 'Beyoncé':\tper:cities_of_residence\n",
            "----------------------------------------------------------------------------------------------------\n",
            "This was among a batch of paperback Oxford World 's Classics that I was given as a reward for\n",
            "reading and commenting on a manuscript for OUP .\n",
            "\n",
            "***No Relations Found***\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The latest investigation was authorized after the Supreme Court in 2007 found DCC and its founder ,\n",
            "Jim Flavin , guilty of selling DCC 's ( EURO ) 106 million ( then $ 130 million ) stake in Fyffes\n",
            "after Flavin --also a Fyffes director at the time -- received inside information about bad Fyffes\n",
            "news in the pipeline .\n",
            "\n",
            "'DCC' -> 'Jim Flavin':\torg:founded_by\n",
            "'DCC' -> 'Flavin':\torg:founded_by\n",
            "'Jim Flavin' -> 'DCC':\tper:employee_of\n",
            "'DCC' -> 'Jim Flavin':\torg:founded_by\n",
            "'DCC' -> 'Flavin':\torg:top_members/employees\n",
            "'Fyffes' -> 'Flavin':\torg:top_members/employees\n",
            "'yffes' -> 'Flavin':\torg:top_members/employees\n",
            "'Fyffes' -> 'Flavin':\torg:top_members/employees\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I just went for a jog and ran into Jimmy who just started at XYZ company.\n",
            "\n",
            "'Jimmy' -> 'XYZ':\tper:employee_of\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "        'Tom Thabane resigned in October last year to form the All Basotho Convention -LRB- ABC -RRB- , crossing the floor with 17 members of parliament , causing constitutional monarch King Letsie III to dissolve parliament and call the snap election .',\n",
        "        \"I just went for a jog and ran into Ben Fischer who is the chief revenue officer of XYZ company.\",\n",
        "        \"Beyoncé lives in Los Angeles.\",\n",
        "        \"This was among a batch of paperback Oxford World 's Classics that I was given as a reward for reading and commenting on a manuscript for OUP .\",\n",
        "        \"The latest investigation was authorized after the Supreme Court in 2007 found DCC and its founder , Jim Flavin , guilty of selling DCC 's -LRB- EURO -RRB- 106 million -LRB- then $ 130 million -RRB- stake in Fyffes after Flavin --also a Fyffes director at the time -- received inside information about bad Fyffes news in the pipeline .\",\n",
        "        \"I just went for a jog and ran into Jimmy who just started at XYZ company.\",\n",
        "        ]\n",
        "\n",
        "print_relations(texts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_relations(\"I met nate cohen and he's the president of the united states of America and his wife joyce is an engineer at google\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzzQXg5KArl1",
        "outputId": "16b11455-f9d0-4443-d28c-51abc6c29660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "I met nate cohen and he's the president of the united states of America and his wife joyce is an\n",
            "engineer at google\n",
            "\n",
            "'nate cohen' -> 'united states of America':\tper:countries_of_residence\n",
            "'nate cohen' -> 'joyce':\tper:spouse\n",
            "'united states of America' -> 'nate cohen':\torg:top_members/employees\n",
            "'united states of America' -> 'joyce':\torg:top_members/employees\n",
            "'joyce' -> 'nate cohen':\tper:spouse\n",
            "'joyce' -> 'united states of America':\tper:countries_of_residence\n",
            "'joyce' -> 'google':\tper:employee_of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_span(doc, word):\n",
        "    start = doc.index(word)\n",
        "    end = start + len(word)\n",
        "    return start, end\n",
        "\n",
        "def check_relation(text, e1, e2):\n",
        "    e1_index = extract_span(text, e1)\n",
        "    e2_index = extract_span(text, e2)\n",
        "    inputs = rel_tokenizer(text, entity_spans=[e1_index, e2_index], return_tensors=\"pt\")\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = rel_model(**inputs)\n",
        "\n",
        "    predicted_index = outputs.logits.argmax(-1)\n",
        "    return rel_model.config.id2label[predicted_index.item()]"
      ],
      "metadata": {
        "id": "-kLZ7WQQZIy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I just went for a jog and ran into Ben Fischer who is the chief revenue officer of XYZ company.\"\n",
        "check_relation(text, \"Ben Fischer\", \"chief revenue officer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oLzV7No1Znqi",
        "outputId": "92be67aa-7c6f-4cc8-c040-e3ab0c6e43c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'per:title'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I went jogging with mike tyson who works as a boxer at Sony and he lives in waltham and travels to miami\"\n",
        "check_relation(text, \"mike tyson\", \"boxer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "waHlWHW5fOw1",
        "outputId": "4d46214c-8881-401c-8fdb-bce14dd7979d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'per:title'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCUv1-kbzTB2"
      },
      "source": [
        "# Ideas for Future Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ18JOKhsYBy"
      },
      "source": [
        "## Biographical\n",
        "\n",
        "Could be interesting to combine these results with a model generated from [Biographical: A Semi-Supervised Relation Extraction Dataset](https://plumaj.github.io/biographical/). This is a dataset annotated for personal information, including: birthdate, birthplace, deathdate, deathplace, occupation, ofParent, educatedAt, hasChild, sibling, other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mQkOdE5sXmX"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1i2Gz_evbO0uXAluoKXOG3C0yrcZvauUs\n",
        "# !tar xvzf tacred_LDC2018T24.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpoMT8AxzXeB"
      },
      "source": [
        "## Question Answering (QA)\n",
        "The paper [Supervised Relation Classification as Two-way Span-Prediction](https://arxiv.org/pdf/2010.04829.pdf) changed the paradigm to predicting spans in based on a QA context. Given a document, they merely ask a predefined set of questions, e.g. What is his role in the company? What company does he work for?, etc. This combined with NER could provide for a very flexible set of relations extracted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj806xa51-tg"
      },
      "source": [
        "## Knowledge Graph\n",
        "Nodes with similar relations can be connected together or nodes with equal entities can be used to predict the presence of other relations. For example, a graph that has two people working at the same company could suggest a coworker relation or propose insights about the two people together."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dygie\n",
        "\n",
        "Implementing the paper \"Entity, Relation, and Event Extraction with Contextualized Span Representations\" [DYGIE](https://github.com/dwadden/dygiepp.git) provides an end-to-end framework (through Spacy) with models focused on span prediction as a step to relation extraction."
      ],
      "metadata": {
        "id": "1VdrT5hez4x1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RzHs0tI43I-l",
        "fRywXM4azmCP",
        "B5sR9PDLtQLv",
        "BX-xzMbY2rWZ",
        "LSRcpqUFMibl"
      ],
      "machine_shape": "hm",
      "name": "David Morley Riley Challenge",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}