{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morleyd/morleyd.github.io/blob/master/Projects/Relation_Extraction_without_Given_Spans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot4A8Ag2zOSb"
      },
      "source": [
        "# Relation Extraction without Given Spans\n",
        "David C. Morley\n",
        "\n",
        "June 6, 2022\n",
        "\n",
        "This work seeks to apply the [TACRED relation classification dataset](https://nlp.stanford.edu/projects/tacred/) reported in [this paper](https://arxiv.org/abs/2010.01057) to the needs of a business focused chatbot. It builds off pretrained models stored oh HuggingFace's Transformers. I selected LUKE, one of the best performing models on this dataset, as ranked by [NLP-Progress](http://nlpprogress.com/english/relationship_extraction.html). I use the [fine-tuned model checkpoint](https://huggingface.co/studio-ousia/luke-large-finetuned-tacred) from Luke, [\"{LUKE}: Deep Contextualized Entity Representations with Entity-aware Self-attention\"](https://aclanthology.org/2020.emnlp-main.523) for handling the relation extraction. However, this model requires the spans of the entities as inputs and only outputs one relation per utterance. To overcome these limitations, I used a Named Entity Recognition (NER) model to exctact all the entity spans relevant to my use case. The NER is handled by [BERT](https://huggingface.co/dslim/bert-base-NER-uncased).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzHs0tI43I-l"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18lGoU-SmTK8"
      },
      "outputs": [],
      "source": [
        "# Currently, LUKE is only available on the master branch\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBERtWDCnqXO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from tqdm import trange\n",
        "from transformers import LukeTokenizer, LukeForEntityPairClassification\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRywXM4azmCP"
      },
      "source": [
        "## Loading the dataset\n",
        "Download from source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLzX8LIS127b"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1-BJPEGPCkcEBgqlEq0WqvwECbOqM0bH3\n",
        "!tar xvzf tacred_LDC2018T24.tgz\n",
        "# Clean up.\n",
        "!rm tacred_LDC2018T24.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5sR9PDLtQLv"
      },
      "source": [
        "### Apply the patch from [TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task](https://github.com/DFKI-NLP/tacrev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDxHXpGHtLdG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/DFKI-NLP/tacrev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nh0PwI0tVfH"
      },
      "outputs": [],
      "source": [
        "!python tacrev/scripts/apply_tacred_patch.py \\\n",
        "  --dataset-file ./tacred/data/json/dev.json \\\n",
        "  --patch-file ./tacrev/patch/dev_patch.json \\\n",
        "  --output-file ./tacred/data/json/dev_rev.json\n",
        "\n",
        "!python tacrev/scripts/apply_tacred_patch.py \\\n",
        "  --dataset-file ./tacred/data/json/test.json \\\n",
        "  --patch-file ./tacrev/patch/test_patch.json \\\n",
        "  --output-file ./tacred/data/json/test_rev.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9KN_AuFui2L"
      },
      "source": [
        "### Process Data\n",
        "\n",
        "Modified from [Luke's source code](https://github.com/studio-ousia/luke/blob/master/examples/relation_classification/reader.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-v01ix-i2FS"
      },
      "outputs": [],
      "source": [
        "def load_examples(dataset_file):\n",
        "    with open(dataset_file, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    examples = []\n",
        "    for i, item in enumerate(data):\n",
        "        tokens = item[\"token\"]\n",
        "        token_spans = dict(\n",
        "            subj=(item[\"subj_start\"], item[\"subj_end\"] + 1),\n",
        "            obj=(item[\"obj_start\"], item[\"obj_end\"] + 1)\n",
        "        )\n",
        "\n",
        "        if token_spans[\"subj\"][0] < token_spans[\"obj\"][0]:\n",
        "            entity_order = (\"subj\", \"obj\")\n",
        "        else:\n",
        "            entity_order = (\"obj\", \"subj\")\n",
        "\n",
        "        text = \"\"\n",
        "        cur = 0\n",
        "        char_spans = {}\n",
        "        for target_entity in entity_order:\n",
        "            token_span = token_spans[target_entity]\n",
        "            text += \" \".join(tokens[cur : token_span[0]])\n",
        "            if text:\n",
        "                text += \" \"\n",
        "            char_start = len(text)\n",
        "            text += \" \".join(tokens[token_span[0] : token_span[1]])\n",
        "            char_end = len(text)\n",
        "            char_spans[target_entity] = (char_start, char_end)\n",
        "            text += \" \"\n",
        "            cur = token_span[1]\n",
        "        text += \" \".join(tokens[cur:])\n",
        "        text = text.rstrip()\n",
        "\n",
        "        examples.append(dict(\n",
        "            text=text,\n",
        "            entity_spans=[tuple(char_spans[\"subj\"]), tuple(char_spans[\"obj\"])],\n",
        "            label=item[\"relation\"]\n",
        "        ))\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H6us8Unl231"
      },
      "outputs": [],
      "source": [
        "test_examples = load_examples(\"/content/tacred/data/json/test_rev.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHw1YtriA3S9"
      },
      "source": [
        "## Loading the fine-tuned model and tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKYVMCBWIrMr"
      },
      "source": [
        "### First get the relation extraction model\n",
        "We construct the model and tokenizer using the [fine-tuned model checkpoint](https://huggingface.co/studio-ousia/luke-large-finetuned-tacred)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9bXAEPZp0ZT"
      },
      "outputs": [],
      "source": [
        "# Load the model checkpoint\n",
        "rel_model = LukeForEntityPairClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")\n",
        "rel_model.eval()\n",
        "rel_model.to(\"cuda\")\n",
        "\n",
        "# Load the tokenizer\n",
        "rel_tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-tacred\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGN6tG22SEIL"
      },
      "source": [
        "### Now get the NER model\n",
        "We construct the model and tokenizer using the [fine-tuned model checkpoint](https://huggingface.co/dslim/bert-base-NER-uncased)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0ePuwXpSEg4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER-uncased\")\n",
        "ner_model.eval()\n",
        "ner_model.to(\"cuda\")\n",
        "\n",
        "ner = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX-xzMbY2rWZ"
      },
      "source": [
        "## Putting both models together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6FjPCkGV1YV"
      },
      "outputs": [],
      "source": [
        "from itertools import permutations\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vXVUz2lO_3u"
      },
      "outputs": [],
      "source": [
        "Entity = namedtuple('Entity', ['e_type', 'span', 'text', 'score'])\n",
        "SPECIAL_TOKENS = {'FLA': '','LCB':'{', 'LRB':'(', 'LSB':'[', 'RCB':'}', 'RRB':')', 'RSB':']'}\n",
        "\n",
        "mean = lambda x: sum(x) / len(x)\n",
        "\n",
        "def clean_text(input_text: str) -> str:\n",
        "    for k,v in SPECIAL_TOKENS.items():\n",
        "        input_text = input_text.replace(f\"-{k}-\",v)\n",
        "    return input_text\n",
        "\n",
        "def merge_ents(ner_results: List[Dict]) -> List[Dict]:\n",
        "    ner_results = sorted(ner_results, key=lambda x: x['start'], reverse=False)\n",
        "    prev_e_type = ''\n",
        "    out = [{'entity':'', 'end': 0, 'start': 1, 'score':0, 'word':''}]\n",
        "    for entity in ner_results:\n",
        "        try:\n",
        "            e_loc, e_type = entity['entity'].split('-')\n",
        "            entity['entity'] = e_type\n",
        "            if e_loc == 'B':\n",
        "                out.append(entity)\n",
        "            elif e_loc == 'I':\n",
        "                out[-1]['end'] = entity['end']\n",
        "                out[-1]['word'] = ' '.join([out[-1]['word'], entity['word']])\n",
        "                out[-1]['score'] = mean([entity['score'], out[-1]['score']])\n",
        "            out[-1]['word'] = out[-1]['word'].replace(' ##', '')\n",
        "            if out[-1]['word'] in SPECIAL_TOKENS.keys():\n",
        "                out.pop()\n",
        "        except ValueError:\n",
        "            if entity['end'] > out[-1]['end']:\n",
        "                out.append(entity)\n",
        "    return out\n",
        "\n",
        "def ent_objects(entities: List[Dict]) -> List[Entity]:\n",
        "    out = []\n",
        "    for e in entities:\n",
        "        out.append(Entity(e['entity'], (e['start'], e['end']), e['word'], e['score']))\n",
        "    return out\n",
        "\n",
        "def ent_spans(entities: List[Dict]) -> List[Tuple]:\n",
        "    out = []\n",
        "    for e in entities:\n",
        "        if e['end'] - e['start'] > 1:\n",
        "            out.append((e['start'], e['end']))\n",
        "    return out\n",
        "\n",
        "def get_span(text: str, span: Tuple[int]) -> str:\n",
        "    return text[span[0]: span[1]]\n",
        "\n",
        "def extract_relations(text: str, ner_results: List[Dict]) -> List[Dict]:\n",
        "    out = []\n",
        "    entities = ent_objects(merge_ents(ner_results))\n",
        "    for e1, e2 in permutations(entities, 2):\n",
        "        inputs = rel_tokenizer(text, entity_spans=[e1.span, e2.span], return_tensors=\"pt\")\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "        outputs = rel_model(**inputs)\n",
        "\n",
        "        predicted_class_idx = outputs.logits.argmax(-1).item()\n",
        "        predicted_label = rel_model.config.id2label[predicted_class_idx]\n",
        "        out.append({'text': text, 'entity_spans': [e1.span, e2.span], 'label': predicted_label})\n",
        "    return out\n",
        "\n",
        "def get_all_spans(ner_results: List[List[Dict]]):  \n",
        "    pred_spans = []\n",
        "    for result in ner_results:\n",
        "        entities = ent_spans(merge_ents(result))\n",
        "        perms = [list(perm) for perm in permutations(entities, 2)]\n",
        "        pred_spans.append(perms)\n",
        "    return pred_spans "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuhdLhSlA93M"
      },
      "source": [
        "## Measuring performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkXNjJNuMb0X"
      },
      "source": [
        "### Base Model\n",
        "This reproduces the evaluation reported in the [original paper](https://arxiv.org/abs/2010.01057) is successfully reproduced. It classifies relations between entity pairs in the test set and measures the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGnyU7rGnNg2",
        "outputId": "d2cf6903-02f5-4121-b498-baf9ba249673"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 122/122 [07:42<00:00,  3.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "precision: 0.7661131438221221 recall: 0.8715978226064681 f1: 0.8154583582983823\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "num_predicted = 0\n",
        "num_gold = 0\n",
        "num_correct = 0\n",
        "\n",
        "for batch_start_idx in trange(0, len(test_examples), batch_size):\n",
        "    batch_examples = test_examples[batch_start_idx:batch_start_idx + batch_size]\n",
        "    texts = [example[\"text\"] for example in batch_examples]\n",
        "    entity_spans = [example[\"entity_spans\"] for example in batch_examples]\n",
        "    gold_labels = [example[\"label\"] for example in batch_examples]\n",
        "    \n",
        "    inputs = rel_tokenizer(texts, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = rel_model(**inputs)\n",
        "    predicted_indices = outputs.logits.argmax(-1)\n",
        "    predicted_labels = [rel_model.config.id2label[index.item()] for index in predicted_indices]\n",
        "    for predicted_label, gold_label in zip(predicted_labels, gold_labels):\n",
        "        if predicted_label != \"no_relation\":\n",
        "            num_predicted += 1\n",
        "        if gold_label != \"no_relation\":\n",
        "            num_gold += 1\n",
        "            if predicted_label == gold_label:\n",
        "                num_correct += 1\n",
        "\n",
        "precision = num_correct / num_predicted\n",
        "recall = num_correct / num_gold\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f\"\\n\\nprecision: {precision} recall: {recall} f1: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSRcpqUFMibl"
      },
      "source": [
        "### My Model\n",
        "The main difference here is that I mimic the data that you'll provide me by not including the spans of the entities. Instead, I use NER to predict the location of the entities and return all the relations between those entities. This has the benefit of giving more than one possible relation per utterance. Unfortunately, this also makes it have different results from the originial test set. I mitigate this by comparing the overlap of the spans that I predict with those provided. If the closest matching span (with at least half overlap) has the same relation, I count it as a positive example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "360d2sNxXHKU"
      },
      "outputs": [],
      "source": [
        "def span_overlap(s1: Tuple[int], s2: Tuple[int]) -> float:\n",
        "    # print(repr(s1))\n",
        "    b1, e1 = s1\n",
        "    b2, e2 = s2\n",
        "    # Check no overlap\n",
        "    if e1 < b2 or b1 > e2:\n",
        "        return 0.\n",
        "    else:\n",
        "        return abs(1 - abs((e2 - e1 + b2 - b1) / (max(e2, e1) - min(b2, b1))))\n",
        "\n",
        "def matching_spans(pred_spans: List[Dict], gold_spans: List[Tuple], k: int) -> List[int]:\n",
        "    \"\"\"For a list of predicted span dictionaries (output of extract_relations)\n",
        "    return the indices of those which most overlap with their gold counterparts\n",
        "    in order of most similar to least\n",
        "    \"\"\"\n",
        "    matches = [i for i, pred in enumerate(pred_spans) if span_overlap(pred[k], gold_spans[k]) > .5]\n",
        "    # return sorted(matches, key=lambda x: span_overlap(pred_spans[x]['entity_spans'][k], gold_spans[k]), reverse=True)\n",
        "    return matches\n",
        "\n",
        "def get_pred_label(pred_spans: Tuple[Tuple], gold_spans: List[Tuple], text):\n",
        "    global TICKER\n",
        "    beg_spans = matching_spans(pred_spans, gold_spans, 0)\n",
        "    end_spans = matching_spans(pred_spans, gold_spans, 1)\n",
        "    if not beg_spans or not end_spans:\n",
        "        TICKER += 1\n",
        "        return [\"no_relation\"]\n",
        "    else:\n",
        "        intersection = np.intersect1d(beg_spans, end_spans)\n",
        "        filtered_spans = [list(pred_spans[i]) for i in intersection]\n",
        "        n = len(filtered_spans)\n",
        "\n",
        "        try:\n",
        "            inputs = rel_tokenizer([text]*n, entity_spans=filtered_spans, return_tensors=\"pt\",padding=True,truncation=True)\n",
        "            inputs = inputs.to(\"cuda\")\n",
        "            with torch.no_grad():\n",
        "                outputs = rel_model(**inputs)\n",
        "\n",
        "            predicted_indices = outputs.logits.argmax(-1)\n",
        "            predicted_labels = [rel_model.config.id2label[index.item()] for index in predicted_indices]\n",
        "        except ValueError:\n",
        "            print(filtered_spans)\n",
        "            predicted_labels = [\"no_relation\"]\n",
        "        return predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMyNdihy_qK9"
      },
      "source": [
        "### LUKE+NER Model Eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En9gsNYmdpQc"
      },
      "source": [
        "This number is taken only out of the samples that had a relation (~%20 of the total ammount). Note the high precision and low recall. This is caused by my method of extracting spans. This task only cared about names, organizations and locations while TACRED's data set looks for more types of relations. However, those that I do select, are selected correctly. My true accuracy on the subset of samples that contain the relevant relations is much higher. My model is also capable of exctracting more relations than the TACRED dataset can account for. This means that I correctly extract far more relations than the dataset knows how to give me credit for.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo_491o25aFf",
        "outputId": "649f26b7-f22e-4f29-bcde-1b6301727406"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 4/122 [00:11<05:39,  2.88s/it]/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py:1015: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  UserWarning,\n",
            "100%|██████████| 122/122 [06:00<00:00,  2.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "precision: 0.7490774907749077 recall: 0.45501120717259047 f1: 0.5661354581673307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "9846"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TICKER = 0\n",
        "batch_size = 128\n",
        "\n",
        "num_predicted = 0\n",
        "num_gold = 0\n",
        "num_correct = 0\n",
        "\n",
        "total = 0\n",
        "num_same = 0\n",
        "\n",
        "for batch_start_idx in trange(0, len(test_examples), batch_size):\n",
        "    batch_examples = test_examples[batch_start_idx:batch_start_idx + batch_size]\n",
        "    texts = [example[\"text\"] for example in batch_examples]\n",
        "    entity_spans = [example[\"entity_spans\"] for example in batch_examples]\n",
        "    gold_labels = [example[\"label\"] for example in batch_examples]\n",
        "\n",
        "    ner_results = ner(texts)\n",
        "    new_spans = get_all_spans(ner_results)\n",
        "\n",
        "    for pred_spans, gold_label, gold_span, text in zip(new_spans, gold_labels, entity_spans, texts):\n",
        "        total += 1\n",
        "        pred_labels = get_pred_label(pred_spans, gold_span, text)\n",
        "        if gold_label in pred_labels:\n",
        "            num_same += 1\n",
        "        if gold_label != \"no_relation\":\n",
        "            num_gold += 1\n",
        "            if gold_label in pred_labels:\n",
        "                num_correct += 1\n",
        "        num_predicted += sum(1 for label in pred_labels if label != \"no_relation\")\n",
        "\n",
        "precision = num_correct / num_predicted\n",
        "recall = num_correct / num_gold\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f\"\\n\\nprecision: {precision} recall: {recall} f1: {f1}\")\n",
        "TICKER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSW1P0C2eVrz"
      },
      "source": [
        "Approx. 86.5% of my predictions matched the gold sample even though almost 2/3 of my predictions defaulted to no relation because the spans I predicted weren't the ones that were annotated (correctly or otherwise). This is just an example of why this is a poor dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjJhTXj9TfCp",
        "outputId": "acc0e2c6-aed6-48bc-984c-9921e80fb3c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.8651105809529951, 0.6348571797021084)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_same / total, TICKER/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut1RDvlb3BLx"
      },
      "source": [
        "## Sandbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSXk4ucreF61"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP26ZoC7Ne6S"
      },
      "outputs": [],
      "source": [
        "wrapper = textwrap.TextWrapper(width=100)\n",
        "\n",
        "def print_relations(text):\n",
        "    if not isinstance(text, list):\n",
        "        text = [text]\n",
        "    ner_results = ner(text)\n",
        "    new_spans = get_all_spans(ner_results)\n",
        "    for doc, result in zip(text, new_spans):\n",
        "        found_relation = False\n",
        "        print('-'*100)\n",
        "        print(wrapper.fill(clean_text(doc)))\n",
        "        print()\n",
        "        \n",
        "        n = len(result)\n",
        "        inputs = rel_tokenizer([doc]*n, entity_spans=result, return_tensors=\"pt\", padding=True)\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = rel_model(**inputs)\n",
        "\n",
        "        predicted_indices = outputs.logits.argmax(-1)\n",
        "        predicted_labels = [rel_model.config.id2label[index.item()] for index in predicted_indices]\n",
        "        for span, label in zip(result, predicted_labels):\n",
        "            if label != 'no_relation':\n",
        "                found_relation = True\n",
        "                print(f\"{repr(get_span(doc, span[0]))} -> {repr(get_span(doc, span[1]))}:\\t{label}\")\n",
        "\n",
        "        if not found_relation:\n",
        "            print(\"***No Relations Found***\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm6pq97dSsgZ",
        "outputId": "6b692c6d-e821-437e-ba30-4ef995ef0d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Tom Thabane resigned in October last year to form the All Basotho Convention ( ABC ) , crossing the\n",
            "floor with 17 members of parliament , causing constitutional monarch King Letsie III to dissolve\n",
            "parliament and call the snap election .\n",
            "\n",
            "'Tom Thabane' -> 'All Basotho':\tper:employee_of\n",
            "'All Basotho' -> 'Tom Thabane':\torg:founded_by\n",
            "'All Basotho' -> 'LRB':\torg:alternate_names\n",
            "'All Basotho' -> 'ABC':\torg:alternate_names\n",
            "'All Basotho' -> 'RRB':\torg:alternate_names\n",
            "'LRB' -> 'All Basotho':\torg:alternate_names\n",
            "'ABC' -> 'Tom Thabane':\torg:founded_by\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I just went for a jog and ran into Ben Fischer who is the chief revenue officer of XYZ company.\n",
            "\n",
            "'Ben Fischer' -> 'XYZ':\tper:employee_of\n",
            "'XYZ' -> 'Ben Fischer':\torg:top_members/employees\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Beyoncé lives in Los Angeles.\n",
            "\n",
            "'Beyoncé' -> 'Los Angeles':\tper:cities_of_residence\n",
            "'Los Angeles' -> 'Beyoncé':\tper:cities_of_residence\n",
            "----------------------------------------------------------------------------------------------------\n",
            "This was among a batch of paperback Oxford World 's Classics that I was given as a reward for\n",
            "reading and commenting on a manuscript for OUP .\n",
            "\n",
            "***No Relations Found***\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The latest investigation was authorized after the Supreme Court in 2007 found DCC and its founder ,\n",
            "Jim Flavin , guilty of selling DCC 's ( EURO ) 106 million ( then $ 130 million ) stake in Fyffes\n",
            "after Flavin --also a Fyffes director at the time -- received inside information about bad Fyffes\n",
            "news in the pipeline .\n",
            "\n",
            "'DCC' -> 'Jim Flavin':\torg:founded_by\n",
            "'DCC' -> 'Flavin':\torg:founded_by\n",
            "'Jim Flavin' -> 'DCC':\tper:employee_of\n",
            "'DCC' -> 'Jim Flavin':\torg:founded_by\n",
            "'DCC' -> 'Flavin':\torg:top_members/employees\n",
            "'Fyffes' -> 'Flavin':\torg:top_members/employees\n",
            "'yffes' -> 'Flavin':\torg:top_members/employees\n",
            "'Fyffes' -> 'Flavin':\torg:top_members/employees\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I just went for a jog and ran into Jimmy who just started at XYZ company.\n",
            "\n",
            "'Jimmy' -> 'XYZ':\tper:employee_of\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "        'Tom Thabane resigned in October last year to form the All Basotho Convention -LRB- ABC -RRB- , crossing the floor with 17 members of parliament , causing constitutional monarch King Letsie III to dissolve parliament and call the snap election .',\n",
        "        \"I just went for a jog and ran into Ben Fischer who is the chief revenue officer of XYZ company.\",\n",
        "        \"Beyoncé lives in Los Angeles.\",\n",
        "        \"This was among a batch of paperback Oxford World 's Classics that I was given as a reward for reading and commenting on a manuscript for OUP .\",\n",
        "        \"The latest investigation was authorized after the Supreme Court in 2007 found DCC and its founder , Jim Flavin , guilty of selling DCC 's -LRB- EURO -RRB- 106 million -LRB- then $ 130 million -RRB- stake in Fyffes after Flavin --also a Fyffes director at the time -- received inside information about bad Fyffes news in the pipeline .\",\n",
        "        \"I just went for a jog and ran into Jimmy who just started at XYZ company.\",\n",
        "        ]\n",
        "\n",
        "print_relations(texts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_relations(\"I met nate cohen and he's the president of the united states of America and his wife joyce is an engineer at google\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzzQXg5KArl1",
        "outputId": "16b11455-f9d0-4443-d28c-51abc6c29660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "I met nate cohen and he's the president of the united states of America and his wife joyce is an\n",
            "engineer at google\n",
            "\n",
            "'nate cohen' -> 'united states of America':\tper:countries_of_residence\n",
            "'nate cohen' -> 'joyce':\tper:spouse\n",
            "'united states of America' -> 'nate cohen':\torg:top_members/employees\n",
            "'united states of America' -> 'joyce':\torg:top_members/employees\n",
            "'joyce' -> 'nate cohen':\tper:spouse\n",
            "'joyce' -> 'united states of America':\tper:countries_of_residence\n",
            "'joyce' -> 'google':\tper:employee_of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_span(doc, word):\n",
        "    start = doc.index(word)\n",
        "    end = start + len(word)\n",
        "    return start, end\n",
        "\n",
        "def check_relation(text, e1, e2):\n",
        "    e1_index = extract_span(text, e1)\n",
        "    e2_index = extract_span(text, e2)\n",
        "    inputs = rel_tokenizer(text, entity_spans=[e1_index, e2_index], return_tensors=\"pt\")\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = rel_model(**inputs)\n",
        "\n",
        "    predicted_index = outputs.logits.argmax(-1)\n",
        "    return rel_model.config.id2label[predicted_index.item()]"
      ],
      "metadata": {
        "id": "-kLZ7WQQZIy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I just went for a jog and ran into Ben Fischer who is the chief revenue officer of XYZ company.\"\n",
        "check_relation(text, \"Ben Fischer\", \"chief revenue officer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oLzV7No1Znqi",
        "outputId": "92be67aa-7c6f-4cc8-c040-e3ab0c6e43c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'per:title'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I went jogging with mike tyson who works as a boxer at Sony and he lives in waltham and travels to miami\"\n",
        "check_relation(text, \"mike tyson\", \"boxer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "waHlWHW5fOw1",
        "outputId": "4d46214c-8881-401c-8fdb-bce14dd7979d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'per:title'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCUv1-kbzTB2"
      },
      "source": [
        "# Ideas for Future Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ18JOKhsYBy"
      },
      "source": [
        "## Biographical\n",
        "\n",
        "Could be interesting to combine these results with a model generated from [Biographical: A Semi-Supervised Relation Extraction Dataset](https://plumaj.github.io/biographical/). This is a dataset annotated for personal information, including: birthdate, birthplace, deathdate, deathplace, occupation, ofParent, educatedAt, hasChild, sibling, other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mQkOdE5sXmX"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1i2Gz_evbO0uXAluoKXOG3C0yrcZvauUs\n",
        "# !tar xvzf tacred_LDC2018T24.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpoMT8AxzXeB"
      },
      "source": [
        "## Question Answering (QA)\n",
        "The paper [Supervised Relation Classification as Two-way Span-Prediction](https://arxiv.org/pdf/2010.04829.pdf) changed the paradigm to predicting spans in based on a QA context. Given a document, they merely ask a predefined set of questions, e.g. What is his role in the company? What company does he work for?, etc. This combined with NER could provide for a very flexible set of relations extracted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj806xa51-tg"
      },
      "source": [
        "## Knowledge Graph\n",
        "Nodes with similar relations can be connected together or nodes with equal entities can be used to predict the presence of other relations. For example, a graph that has two people working at the same company could suggest a coworker relation or propose insights about the two people together."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dygie\n",
        "\n",
        "Implementing the paper \"Entity, Relation, and Event Extraction with Contextualized Span Representations\" [DYGIE](https://github.com/dwadden/dygiepp.git) provides an end-to-end framework (through Spacy) with models focused on span prediction as a step to relation extraction."
      ],
      "metadata": {
        "id": "1VdrT5hez4x1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RzHs0tI43I-l",
        "fRywXM4azmCP",
        "B5sR9PDLtQLv",
        "BX-xzMbY2rWZ",
        "LSRcpqUFMibl"
      ],
      "machine_shape": "hm",
      "name": "Relation Extraction without Given Spans",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}